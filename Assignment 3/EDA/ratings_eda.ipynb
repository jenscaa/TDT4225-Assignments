{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a7f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f749336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26024289 entries, 0 to 26024288\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   userId     int64  \n",
      " 1   movieId    int64  \n",
      " 2   rating     float64\n",
      " 3   timestamp  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 794.2 MB\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_csv('../data/ratings.csv')\n",
    "ratings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017de454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100004 entries, 0 to 100003\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   userId     100004 non-null  int64  \n",
      " 1   movieId    100004 non-null  int64  \n",
      " 2   rating     100004 non-null  float64\n",
      " 3   timestamp  100004 non-null  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "ratings_small = pd.read_csv('../data/ratings_small.csv')\n",
    "ratings_small.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffdf823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbb9b0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ratings files...\n",
      "Small ratings shape: (100004, 4)\n",
      "Full ratings shape: (26024289, 4)\n",
      "\n",
      "Analyzing user overlap...\n",
      "Number of matching records: 99676\n",
      "Percentage of small ratings found in full ratings: 99.67%\n",
      "\n",
      "Users in small_ratings: 671\n",
      "Users from small_ratings found in full_ratings: 670\n",
      "Users from small_ratings NOT found in full_ratings: 1\n",
      "Unmatched users: {np.int64(519)}\n",
      "\n",
      "First 10 unmatched user IDs: [np.int64(519)]\n",
      "\n",
      "Sample records from unmatched users:\n",
      "       userId  movieId  rating   timestamp       composite_key\n",
      "74759     519       50     4.0  1468758676   50_4.0_1468758676\n",
      "74760     519      104     4.0  1469927080  104_4.0_1469927080\n",
      "74761     519      111     4.0  1468758621  111_4.0_1468758621\n",
      "74762     519      216     4.0  1469927082  216_4.0_1469927082\n",
      "74763     519      223     4.5  1468759080  223_4.5_1468759080\n",
      "74764     519      235     4.0  1468758651  235_4.0_1468758651\n",
      "74765     519      260     5.0  1468928013  260_5.0_1468928013\n",
      "74766     519      288     4.5  1468927376  288_4.5_1468927376\n",
      "74767     519      296     4.5  1468927377  296_4.5_1468927377\n",
      "74768     519      318     4.5  1468759064  318_4.5_1468759064\n",
      "\n",
      "Maximum userId in full_ratings: 270896\n",
      "Assigning new IDs to 1 unmatched users:\n",
      "  User 519 -> User 270897\n",
      "\n",
      "Final merged dataset shape: (100009, 4)\n",
      "Unique users in merged dataset: 676\n",
      "Maximum userId in merged dataset: 270897\n",
      "âœ“ No duplicate (userId, movieId, timestamp) rows.\n",
      "\n",
      "Merged dataset saved to '../data/merged_ratings.csv'\n",
      "\n",
      "==================================================\n",
      "DETAILED ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Rating distribution in small_ratings:\n",
      "rating\n",
      "0.5     1101\n",
      "1.0     3326\n",
      "1.5     1687\n",
      "2.0     7271\n",
      "2.5     4449\n",
      "3.0    20064\n",
      "3.5    10538\n",
      "4.0    28750\n",
      "4.5     7723\n",
      "5.0    15095\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Rating distribution in full_ratings:\n",
      "rating\n",
      "0.5     404897\n",
      "1.0     843310\n",
      "1.5     403607\n",
      "2.0    1762440\n",
      "2.5    1255358\n",
      "3.0    5256722\n",
      "3.5    3116213\n",
      "4.0    6998802\n",
      "4.5    2170441\n",
      "5.0    3812499\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Rating distribution in merged dataset:\n",
      "rating\n",
      "0.5     1101\n",
      "1.0     3327\n",
      "1.5     1687\n",
      "2.0     7271\n",
      "2.5     4449\n",
      "3.0    20064\n",
      "3.5    10538\n",
      "4.0    28752\n",
      "4.5     7723\n",
      "5.0    15097\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Movie analysis:\n",
      "Movies in small_ratings: 9066\n",
      "Movies in full_ratings: 45115\n",
      "Movies from small_ratings found in full_ratings: 9023\n",
      "Percentage of small movies found in full: 99.53%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def merge_ratings_files():\n",
    "    \"\"\"\n",
    "    Merge ratings_small.csv and ratings.csv.\n",
    "\n",
    "    - Row-level matches are detected by 'composite_key' (movieId, rating, timestamp).\n",
    "    - For users that exist in full_ratings, ALL their small_ratings rows (matched or not)\n",
    "      are mapped to their full userId.\n",
    "    - Users that don't exist in full_ratings get new userIds starting after the max in full.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading ratings files...\")\n",
    "\n",
    "    small_ratings = pd.read_csv('../data/ratings_small.csv')\n",
    "    print(f\"Small ratings shape: {small_ratings.shape}\")\n",
    "\n",
    "    full_ratings = pd.read_csv('../data/ratings.csv')\n",
    "    print(f\"Full ratings shape: {full_ratings.shape}\")\n",
    "\n",
    "    # Build composite keys (format rating to 1 decimal to be safe)\n",
    "    small_ratings['composite_key'] = (\n",
    "        small_ratings['movieId'].astype(str) + '_' +\n",
    "        small_ratings['rating'].map(lambda x: f\"{float(x):.1f}\") + '_' +\n",
    "        small_ratings['timestamp'].astype(str)\n",
    "    )\n",
    "    full_ratings['composite_key'] = (\n",
    "        full_ratings['movieId'].astype(str) + '_' +\n",
    "        full_ratings['rating'].map(lambda x: f\"{float(x):.1f}\") + '_' +\n",
    "        full_ratings['timestamp'].astype(str)\n",
    "    )\n",
    "\n",
    "    print(\"\\nAnalyzing user overlap...\")\n",
    "\n",
    "    # Inner join on composite_key; overlapping cols get suffixes\n",
    "    matches = small_ratings.merge(\n",
    "        full_ratings, on='composite_key', how='inner', suffixes=('_small', '_full')\n",
    "    )\n",
    "\n",
    "    print(f\"Number of matching records: {len(matches)}\")\n",
    "    print(f\"Percentage of small ratings found in full ratings: {len(matches)/len(small_ratings)*100:.2f}%\")\n",
    "\n",
    "    small_users = set(small_ratings['userId'].unique())\n",
    "    matched_users = set(matches['userId_small'].unique()) if len(matches) else set()\n",
    "    unmatched_users = small_users - matched_users\n",
    "\n",
    "    print(f\"\\nUsers in small_ratings: {len(small_users)}\")\n",
    "    print(f\"Users from small_ratings found in full_ratings: {len(matched_users)}\")\n",
    "    print(f\"Users from small_ratings NOT found in full_ratings: {len(unmatched_users)}\")\n",
    "    print(f\"Unmatched users: {unmatched_users}\")\n",
    "\n",
    "    if unmatched_users:\n",
    "        first10 = sorted(list(unmatched_users))[:10]\n",
    "        print(f\"\\nFirst 10 unmatched user IDs: {first10}\")\n",
    "        print(\"\\nSample records from unmatched users:\")\n",
    "        print(small_ratings[small_ratings['userId'].isin(unmatched_users)].head(10))\n",
    "\n",
    "    # Build a stable mapping from userId_small -> userId_full using matches\n",
    "    # If (pathologically) a small user maps to multiple full users, pick the most common\n",
    "    if len(matches) > 0:\n",
    "        pairs = matches[['userId_small', 'userId_full']]\n",
    "        mapping = {}\n",
    "        for uid_small, group in pairs.groupby('userId_small'):\n",
    "            counts = Counter(group['userId_full'])\n",
    "            mapping[uid_small] = counts.most_common(1)[0][0]  # pick the modal mapping\n",
    "    else:\n",
    "        mapping = {}\n",
    "\n",
    "    # 1) Matched rows: take FULL versions for all fields\n",
    "    if len(matches) > 0:\n",
    "        matched_part = matches[['movieId_full', 'rating_full', 'timestamp_full', 'userId_full']].copy()\n",
    "        matched_part.rename(columns={\n",
    "            'movieId_full': 'movieId',\n",
    "            'rating_full': 'rating',\n",
    "            'timestamp_full': 'timestamp',\n",
    "            'userId_full': 'userId'\n",
    "        }, inplace=True)\n",
    "    else:\n",
    "        matched_part = pd.DataFrame(columns=['movieId', 'rating', 'timestamp', 'userId'])\n",
    "\n",
    "    # 2) Unmatched rows from small by composite key\n",
    "    unmatched_rows = small_ratings[~small_ratings['composite_key'].isin(matches['composite_key'])] if len(matches) > 0 else small_ratings.copy()\n",
    "\n",
    "    # Split unmatched rows into:\n",
    "    #   a) belong to users that exist in full (we can map their userId via 'mapping')\n",
    "    #   b) belong to users that don't exist in full (we will assign new userIds)\n",
    "    unmatched_have_map = unmatched_rows[unmatched_rows['userId'].isin(mapping.keys())].copy()\n",
    "    unmatched_no_map = unmatched_rows[~unmatched_rows['userId'].isin(mapping.keys())].copy()\n",
    "\n",
    "    # a) Map userId via mapping (keep small's movie/rating/timestamp)\n",
    "    if len(unmatched_have_map) > 0:\n",
    "        unmatched_have_map = unmatched_have_map[['movieId', 'rating', 'timestamp', 'userId']].copy()\n",
    "        unmatched_have_map['userId'] = unmatched_have_map['userId'].map(mapping)\n",
    "\n",
    "    # b) Assign new userIds (only for users not in full at all)\n",
    "    if len(unmatched_no_map) > 0:\n",
    "        max_user_id = int(full_ratings['userId'].max())\n",
    "        print(f\"\\nMaximum userId in full_ratings: {max_user_id}\")\n",
    "\n",
    "        new_ids = {}\n",
    "        next_id = max_user_id + 1\n",
    "        for old_uid in sorted(unmatched_users):\n",
    "            new_ids[int(old_uid)] = next_id\n",
    "            next_id += 1\n",
    "\n",
    "        print(f\"Assigning new IDs to {len(new_ids)} unmatched users:\")\n",
    "        for i, (old_id, new_id) in enumerate(new_ids.items()):\n",
    "            if i < 10:\n",
    "                print(f\"  User {old_id} -> User {new_id}\")\n",
    "            elif i == 10:\n",
    "                print(\"  ...\")\n",
    "\n",
    "        unmatched_no_map = unmatched_no_map[['movieId', 'rating', 'timestamp', 'userId']].copy()\n",
    "        unmatched_no_map['userId'] = unmatched_no_map['userId'].map(new_ids).astype('Int64')\n",
    "\n",
    "        # Drop any rows for which we somehow didn't create a new ID (safety)\n",
    "        unmatched_no_map = unmatched_no_map.dropna(subset=['userId'])\n",
    "\n",
    "    # Combine all parts\n",
    "    parts = [df for df in [matched_part, unmatched_have_map, unmatched_no_map] if df is not None and len(df) > 0]\n",
    "    final_merged = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame(columns=['movieId', 'rating', 'timestamp', 'userId'])\n",
    "\n",
    "    # Enforce dtypes\n",
    "    for col in ['movieId', 'timestamp', 'userId']:\n",
    "        if col in final_merged.columns and final_merged[col].dtype != 'int64':\n",
    "            final_merged[col] = final_merged[col].astype('int64')\n",
    "    if 'rating' in final_merged.columns:\n",
    "        final_merged['rating'] = final_merged['rating'].astype('float32')\n",
    "\n",
    "    print(f\"\\nFinal merged dataset shape: {final_merged.shape}\")\n",
    "    print(f\"Unique users in merged dataset: {final_merged['userId'].nunique()}\")\n",
    "    print(f\"Maximum userId in merged dataset: {final_merged['userId'].max()}\")\n",
    "\n",
    "    # Correct duplicate check: duplicated (userId, movieId, timestamp)\n",
    "    dup_mask = final_merged.duplicated(subset=['userId', 'movieId', 'timestamp'], keep=False)\n",
    "    num_dups = int(dup_mask.sum())\n",
    "    if num_dups > 0:\n",
    "        print(f\"WARNING: Found {num_dups} duplicate (userId, movieId, timestamp) rows.\")\n",
    "        print(final_merged[dup_mask].head())\n",
    "    else:\n",
    "        print(\"âœ“ No duplicate (userId, movieId, timestamp) rows.\")\n",
    "\n",
    "    # Save\n",
    "    final_merged.to_csv('../data/merged_ratings.csv', index=False)\n",
    "    print(\"\\nMerged dataset saved to '../data/merged_ratings.csv'\")\n",
    "\n",
    "    # Extra analysis\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DETAILED ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(\"\\nRating distribution in small_ratings:\")\n",
    "    print(small_ratings['rating'].value_counts().sort_index())\n",
    "\n",
    "    print(\"\\nRating distribution in full_ratings:\")\n",
    "    print(full_ratings['rating'].value_counts().sort_index())\n",
    "\n",
    "    print(\"\\nRating distribution in merged dataset:\")\n",
    "    print(final_merged['rating'].value_counts().sort_index())\n",
    "\n",
    "    # Movie overlap analysis\n",
    "    small_movies = set(small_ratings['movieId'].unique())\n",
    "    full_movies = set(full_ratings['movieId'].unique())\n",
    "    matched_movies = set(matches['movieId_full'].unique()) if len(matches) > 0 else set()\n",
    "\n",
    "    print(f\"\\nMovie analysis:\")\n",
    "    print(f\"Movies in small_ratings: {len(small_movies)}\")\n",
    "    print(f\"Movies in full_ratings: {len(full_movies)}\")\n",
    "    print(f\"Movies from small_ratings found in full_ratings: {len(matched_movies)}\")\n",
    "    print(f\"Percentage of small movies found in full: {len(matched_movies)/len(small_movies)*100:.2f}%\")\n",
    "\n",
    "    return final_merged, matches, unmatched_users\n",
    "\n",
    "merged_data, matches, unmatched_users = merge_ratings_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d22d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Check for duplicates in the merged dataset\n",
    "print(merged_data.duplicated(subset=['userId', 'movieId', 'timestamp']).sum())\n",
    "#Check for duplicates in the small dataset\n",
    "print(ratings_small.duplicated(subset=['userId', 'movieId', 'timestamp']).sum())\n",
    "#Check for duplicates in the full dataset\n",
    "print(ratings.duplicated(subset=['userId', 'movieId', 'timestamp']).sum())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
